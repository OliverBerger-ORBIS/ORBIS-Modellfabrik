## Agenten-Testpolicy: Produktionscode nicht fÃ¼r Alt-Tests verbiegen

Wenn ein Test nach einer beabsichtigten Ã„nderung am produktiven Verhalten (z. B. Icon-Mapping im Asset-Manager) rot wird, gilt:

- Passe die betroffenen Tests an das neue, gewollte Verhalten an.
- Ã„ndere NICHT den produktiven Code nur, um bestehende (veraltete) Tests grÃ¼n zu bekommen.
- Dokumentiere die Testanpassung im PR kurz (Was/Warum), verweise auf die zugehÃ¶rige Ã„nderung (z. B. Guide/ADR).

BegrÃ¼ndung: Tests spiegeln das gewÃ¼nschte Verhalten wider. Wenn sich die fachliche Entscheidung Ã¤ndert (z. B. neue SVGs), mÃ¼ssen die Tests diese neue Wahrheit abbilden â€“ nicht umgekehrt.

# Testing Strategy Guide

**Zielgruppe:** Entwickler  
**Letzte Aktualisierung:** 20.09.2025

## ğŸ¯ Test-Philosophie

### **Test-First Development**
1. **Implementierung** â†’ Test â†’ Fix â†’ Test â†’ Commit
2. **Nur bei 100% funktionierenden Features** committen
3. **Tests haben absolute PrioritÃ¤t vor Commits**

### **KEINE COMMITS VOR TESTS**
- **NIEMALS** Commits durchfÃ¼hren bevor alle Implementierungen vollstÃ¤ndig getestet wurden
- Jede neue FunktionalitÃ¤t muss erst funktional getestet werden
- **UI-Tests** werden vom Benutzer durchgefÃ¼hrt und Ergebnisse mitgeteilt

## ğŸ§ª Test-Kategorien

### **1. Unit-Tests**
- **Ziel:** Automatische Tests fÃ¼r einzelne Funktionen
- **Tools:** pytest, unittest
- **Coverage:** Mindestens 80% Code-Coverage
- **AusfÃ¼hrung:** `python -m pytest tests_orbis/`

### **2. Integration-Tests**
- **Ziel:** Tests fÃ¼r Komponenten-Interaktion
- **Fokus:** MQTT-Integration, Manager-Interaktion
- **Mocking:** MQTT-Client, Session State
- **AusfÃ¼hrung:** `python -m pytest tests_orbis/test_omf/`

### **3. UI-Tests (KRITISCH)**
- **Ziel:** Manuelle Tests der BenutzeroberflÃ¤che
- **DurchfÃ¼hrung:** Vom Benutzer durchgefÃ¼hrt
- **Kriterien:**
  - Dashboard startet ohne Fehler
  - Features funktionieren wie erwartet
  - Keine Runtime-Fehler
  - UI ist bedienbar

### **4. Performance-Tests**
- **Ziel:** Memory-Leaks und Performance-Probleme erkennen
- **Tools:** memory_profiler, cProfile
- **Kriterien:** Keine Memory-Leaks, akzeptable Response-Zeiten

## ğŸ”§ Test-Tools

### **pytest Configuration**
```python
# pytest.ini
[tool:pytest]
testpaths = tests_orbis
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = --cov=omf --cov-report=html --cov-report=term
```

### **Test-Directory Structure**
```
tests_orbis/
â”œâ”€â”€ test_omf/                    # OMF-spezifische Tests
â”‚   â”œâ”€â”€ test_dashboard/          # Dashboard-Tests
â”‚   â”œâ”€â”€ test_tools/              # Tools-Tests
â”‚   â””â”€â”€ test_aps_integration/    # APS-Integration-Tests
â”œâ”€â”€ test_helper_apps/            # Helper-App-Tests
â””â”€â”€ conftest.py                  # Pytest-Fixtures
```

### **Mock-Objekte**
```python
# conftest.py
@pytest.fixture
def mock_mqtt_client():
    """Mock MQTT-Client fÃ¼r Tests"""
    client = Mock()
    client.subscribe_many = Mock()
    client.get_buffer = Mock(return_value=[])
    client.publish = Mock(return_value=True)
    return client

@pytest.fixture
def mock_streamlit():
    """Mock Streamlit fÃ¼r UI-Tests"""
    with patch('streamlit.container'), \
         patch('streamlit.columns') as mock_columns:
        mock_columns.return_value = [Mock(), Mock()]
        yield
```

## ğŸ“‹ Test-Checkliste

### **Vor jedem Commit**
- [ ] **Unit-Tests:** Alle Funktionen getestet
- [ ] **Integration-Tests:** Komponenten-Interaktion funktioniert
- [ ] **UI-Tests:** BenutzeroberflÃ¤che vollstÃ¤ndig bedienbar
- [ ] **Performance-Tests:** Keine Memory-Leaks oder Performance-Probleme
- [ ] **Code-Coverage:** Mindestens 80%
- [ ] **Linting:** Black, Ruff ohne Fehler
- [ ] **Pre-commit Hooks:** Alle Hooks erfolgreich

### **UI-Test-Checkliste**
- [ ] **Session Manager:** Graph-Visualisierung, alle Tabs funktional
- [ ] **OMF-Dashboard:** Module Control, alle Komponenten laden
- [ ] **Logging-System:** Keine Spam-Logs, saubere Ausgabe
- [ ] **MQTT-Integration:** Nachrichten werden korrekt angezeigt
- [ ] **APS-Integration:** APS-Tabs funktionieren
- [ ] **Error Handling:** Fehler werden korrekt behandelt

## ğŸš€ Test-Automatisierung

### **Pre-commit Hooks**
```yaml
# .pre-commit-config.yaml
repos:
  - repo: local
    hooks:
      - id: pytest
        name: Run Tests
        entry: python -m pytest tests_orbis/
        language: system
        files: \.py$
      
      - id: black
        name: Format Code
        entry: black --line-length 120 .
        language: system
        files: \.py$
      
      - id: ruff
        name: Lint Code
        entry: ruff check .
        language: system
        files: \.py$
```

### **CI/CD Pipeline**
```yaml
# .github/workflows/test.yml
name: Tests
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.8
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Run tests
        run: python -m pytest tests_orbis/ --cov=omf
```

## ğŸ“Š Test-Metriken

### **Code-Coverage**
- **Ziel:** Mindestens 80% Code-Coverage
- **Tools:** pytest-cov, coverage.py
- **AusfÃ¼hrung:** `python -m pytest --cov=omf --cov-report=html`

### **Test-Performance**
- **Ziel:** Tests laufen in unter 30 Sekunden
- **Tools:** pytest-benchmark
- **AusfÃ¼hrung:** `python -m pytest --benchmark-only`

### **Test-QualitÃ¤t**
- **Ziel:** Weniger als 5% fehlgeschlagene Tests
- **Tools:** pytest-html, pytest-xdist
- **AusfÃ¼hrung:** `python -m pytest --html=report.html --self-contained-html`

## ğŸ” Debugging Tests

### **Test-Debugging**
```python
# Test mit Debug-Output
def test_my_function():
    result = my_function()
    print(f"Debug: result = {result}")  # Debug-Output
    assert result == expected

# Test mit PDB
def test_my_function():
    result = my_function()
    import pdb; pdb.set_trace()  # Breakpoint
    assert result == expected
```

### **Test-Logging**
```python
# Test mit Logging
def test_my_function(caplog):
    with caplog.at_level(logging.INFO):
        result = my_function()
    
    assert "Expected log message" in caplog.text
    assert result == expected
```

## ğŸ“š Test-Dokumentation

### **Test-Dokumentation**
- **Docstrings** fÃ¼r alle Test-Funktionen
- **Kommentare** fÃ¼r komplexe Test-Logik
- **README** fÃ¼r Test-Setup und -AusfÃ¼hrung

### **Test-Examples**
```python
def test_stock_manager_inventory_update():
    """Test fÃ¼r StockManager.update_inventory()
    
    Testet:
    - Inventory-Update mit gÃ¼ltigen Daten
    - Fehlerbehandlung bei ungÃ¼ltigen Daten
    - Logging-Verhalten
    """
    manager = StockManager()
    
    # Test gÃ¼ltige Inventory-Update
    inventory_data = {"A1": "RED", "B2": "BLUE"}
    result = manager.update_inventory(inventory_data)
    
    assert result == True
    assert len(manager.orders) == 1
    assert manager.orders[0]["id"] == "test-001"
```

---

*Teil der OMF-Dokumentation | [ZurÃ¼ck zur README](../../README.md)*
